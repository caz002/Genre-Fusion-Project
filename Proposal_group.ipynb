{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Project Proposal - Lofi Hip-Hop Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Brandon Kao\n",
    "- Kyle Vu\n",
    "- Connie Chang\n",
    "- Catherine Zhang\n",
    "- Grace Yang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "Recent developments in technology have focused on using AI to create original works in art or writing. Our research explores using AI generation to produce and compose new music, with a focus on replicating works in the genre lo-fi hip hop. We will be creating an AI generator that learns patterns in music and produces a coherent MIDI track as an output. The dataset used for training and testing will consist of MIDI and MP3 files. MIDI files contain information about the structure of a music track, such as the notes being played or the tempo, while MP3s consist of encoded audio data. We will be cutting the MIDI and MP3 files in order to decrease the size of each individual datapoint and to introduce more variety into our dataset. Since we cannot employ Inception Score used in evaluating models for creative artworks, we will instead grade the performance of the model based on the pitch count, pitch range, average pitch interval, and average note length of different outputs. Through employing different types of audio data and validation measures, we hope to improve the musical quality of AI generated works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "In the past few years, there has been a rapid development in using AI for generating media and music. OpenAI, the company behind ChatGPT and DALL-E for text and image generation, has also created a tool called MuseNet, which can make up to 4-minute long music compositions in various music styles<a name=\"openai\"></a>[<sup>[1]</sup>](#openainote). In 2015, Jukedeck emerged as a website where users could use AI to create royalty-free music<a name=\"juke1\"></a>[<sup>[2]</sup>](#juke1note). Jukedeck rose to fame when it was used to produce background music for companies like Google and Coca-Cola<a name=\"juke2\"></a>[<sup>[3]</sup>](#juke2note), and was later bought by ByteDance, the parent company for Tiktok<a name=\"juke3\"></a>[<sup>[4]</sup>](#juke3note). Google’s Magenta is an open source project that provides AI tools to musicians and artists to utilize in their work<a name=\"magenta\"></a>[<sup>[5]</sup>](#magentanote). A growing base of tech corporations are developing the means for AI to generate music, and are looking to push their tools into the creative development space.\n",
    "\n",
    "According to Techopedia, most AI algorithms process music through tokenizing elements of MIDI and audio files for the algorithm to read, and detect patterns on the pitch and rhythm to predict the following sequence<a name=\"tech\"></a>[<sup>[6]</sup>](#technote).Emerging are musicians employing AI generators in their work, such as Taryn Southern, who in an interview with The Verge described how she cut sounds from the AI software Amper Music to piece together tracks for her album I AM AI<a name=\"plaugic\"></a>[<sup>[7]</sup>](#plaugicnote). At its current state of development, AI compositions can be a tool for artists to use, but without human input and editing afterwards, they struggle in being aesthetically and emotionally pleasing works. Creating compositions of music through a computer algorithm has a unique set of challenges, in that it can be hard for an AI to create a cohesive tempo, dynamic, and pitch over a long period of time.\n",
    "\n",
    "Lo-fi hip hop is a subgenre that emerged in the 2010s that uses sampling from jazz piano and hip hop beats, and is largely associated with studying and background music on YouTube<a name=\"sherburne\"></a>[<sup>[8]</sup>](#sherbnote). The user Zachary published on Medium his experiments with creating an AI lo-fi hip hop music generator, by using a RNN prediction model<a name=\"zachary\"></a>[<sup>[9]</sup>](#zachnote). Zachary describes his process of taking the MIDI output from his generator and editing the sound to better fit the genre<a name=\"zachary\"></a>[<sup>[9]</sup>](#zachnote). Zachary is one of many individual attempts made to replicate a music genre in AI that still requires editing the final output to produce a coherent piece.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Creating an AI that can generate lo-fi music is difficult due to a limited understanding of the underlying features that define the genre. Lo-fi music, characterized by its \"low-fidelity\" nature, includes intentional imperfections such as misplayed notes, environmental noise, and audio anomalies similar to older phonographic recordings. It can be hard for a computer algorithm to determine the components that create lo-fi music due to the wide range of noises, styles, and instruments used. Music requires randomization of such variables for creativity and variety, but also structure and repetition to create melodies and harmonies. Decisions about a piece are often made at the composer’s discretion and using their personal judgment on what is aesthetically pleasing, which can be difficult to encode. We will attempt to quantify and measure the quality of music by tracking variables like pitch and variety of note lengths. However, creating a model that can learn and replicate how music producers make songwriting decisions is still challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "For our model-training process, we are considering two potential types of data to use as the training dataset: MP3 and MIDI files. Both file types have advantages and disadvantages, so we are currently in the process of deciding which type to go with. MIDI files are a unique kind of audio file which contain information about the notes, timing, pitch, and other related variables that serves as an instruction manual for how a song is performed. MIDI files are much less complex than MP3s and so are easier to work with and analyze. However, from our background research into potential datasets, we struggled to find MIDI datasets of a sufficient size for the scope of this project. MP3 files are a commonly used audio compression format, containing actual waveform data of songs. Because it is so well-established, song data in an MP3 format is easily accessible. However, working with MP3 data is much more complex as it involves waveform analysis. We will discuss potential datasets for our project below.\n",
    "\n",
    "If we decide to base our research in analyzing MIDI files, one dataset we could use is Soumik Rakshit’s dataset “Classical Music MIDI,” which holds 295 midi files from 19 famous piano composers, such as Chopin and Bach<a name=\"classic\"></a>[<sup>[12]</sup>](#classicnote) . The MIDI files would need to be preprocessed into a matrix, with the rows representing a note, and the column corresponding to the second in the song at which the note was played. To provide more information about the tempo and key of the piece, we would also need to store the note lengths and pitch class of the music using transition matrices. To understand more about the variety in the notes played, we can use a histogram to analyze the layout of the note counts and frequency in a piece, and compare it to histograms analyzing other pieces. We will use transformation matrices and histograms to categorize the various components that make up a song. \n",
    "\n",
    "If we decide to focus primarily on MP3 data for training our model, we could compile a dataset using the official Lofi Girl collection, which has ~3000 MP3 files publicly available for download<a name=\"lgirl\"></a>[<sup>[13]</sup>](#lgirlnote). Each song would be considered its own observation, and within each song every frequency sample would be considered a variable. Since working directly with MP3 data is quite complex, we would likely need a significant amount of preprocessing before we can use this data to train our lofi model. This process would likely involve computing a Fourier transform of each waveform observation in order to decompose it into frequency components. Then, we could use these Fourier transforms to generate spectrograms, which would reveal features of the data such as individual notes or underlying melodies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Our project's objective is to create a machine learning model that can generate lofi music tracks. We will use unsupervised learning techniques such as dimensionality reduction to learn about the embeddings of lofi music, aiming to quantify and measure the genre's characteristics. This process involves identifying patterns or features within the music's embeddings and leveraging these insights for music generation and genre analysis. The project's success will be evaluated based on the model's capability to produce identifiable lofi music and the ability of the unsupervised methods to uncover meaningful and consistent patterns within the music's embeddings. The project utilizes a Generative Adversarial Network (GAN) for creating lofi music. This method is promising because GANs have demonstrated proficiency in generating realistic outputs across various domains. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Unlike image-generating GANs, music-generating models do not have a common benchmark or metrics. The common Inception Score or Frechet Inception Distance, which are often used to evaluate performance in AI image generation, cannot be used to measure the quality of AI-generated music<a name=\"yang\"></a>[<sup>[11]</sup>](#yangnote). As researcher Li-Chia Yang from Georgia Institute of Technology elaborates, “Evaluation of [music] generative models has been falling behind the system development itself”<a name=\"yang\"></a>[<sup>[10]</sup>](#yangnote). That being said, scientists have created toolboxes to create metrics for music data analysis<a name=\"yang\"></a>[<sup>[10]</sup>](#yangnote). We will utilize pitch count, pitch class histogram, pitch class transition matrices, pitch range, average pitch interval, note count, average inter-onset-interval, note length histogram, and note length transition matrices to evaluate the success of our generative model. We will also observe which variables or factors make the most impact in creating the best performance model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we note that our model will only be trained on publicly sourced data, and in our report we will make sure to give proper credit to the person who compiled the original dataset as well as the artists who produced the songs in the dataset.\n",
    "\n",
    "In our initial data cleaning process, we will make sure to examine the data for potential biases. This might include preventing an overrepresentation of specific artists or genres to make sure our model fairly represents the type of music styles we plan to showcase. When we are selecting our model-training method, we will take care to make sure we pick a method that is properly able to represent the data in an accurate manner.\n",
    "\n",
    "One potential ethical concern with our project stems from the fact that our model will be trained on human-produced songs. Naturally, we would not want our trained model to encroach on the human creative space or devalue the music produced by these artists. However, as we have no plans to distribute any music produced by this model or pass the model’s work off as our own creative output, we believe we can properly avoid these concerns. This project is simply meant for us to explore how artificial intelligence can be used in a creative setting, and can show us some interesting trends in the music we listen to.\n",
    "\n",
    "Upon completion of our music-making model, we will make sure to present our results in an unbiased manner and not attempt to make any overreaching conclusions in our report. We will make sure to address all of the shortcomings of our approach and the limitations in the data to paint our conclusions in the most accurate possible light.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* All group members will communicate via Discord. This means sharing project ideas, discussing our availability, allocating tasks, asking for help, and offering advice to each other.\n",
    "* Every group member will contribute equally to the project. \n",
    "* We will set clear deadlines for each portion of the project and complete our assigned tasks by said deadlines.\n",
    "* We will review each other’s work and hold each other accountable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace this with something meaningful that is appropriate for your needs. It doesn't have to be something that fits this format.  It doesn't have to be set in stone... \"no battle plan survives contact with the enemy\". But you need a battle plan nonetheless, and you need to keep it updated so you understand what you are trying to accomplish, who's responsible for what, and what the expected due dates are for each item.\n",
    "\n",
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/15  |  10 PM |  Read over proposal, brainstorm topics/questions  | Discuss and decide on final project topic, research for potential datasets, allocate proposal sections for completion | \n",
    "| 2/20  |  3 PM | Work on individual sections of proposal  | Discuss proposal sections, decide on project dataset(Brandon), finalize proposal for submission | \n",
    "| 2/27  | 3PM  | Begin data cleaning/preprocessing(Grace)  | Finalize training dataset(Catherine), begin model training(Connie)   |\n",
    "| 3/5  | 3 PM  | Finish model training(Kyle) | Generate music using trained model(Brandon), discuss outcomes, refine model, begin writing final project sections(all)  |\n",
    "| 3/12  | 3 PM  | Work on sections for final project submission | Finish project sections, turn in final project |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"openai\"></a>1.[^](#openainote) Payne, Christine. \"MuseNet.\" OpenAI, 25 Apr. 2019, openai.com/blog/musenet<br>\n",
    "<a name=\"juke1\"></a>2.[^](#openainote)Constine, Josh. “Need Music For A Video? Jukedeck’s AI Composer Makes Cheap, Custom Soundtracks.” TechCrunch, Yahoo, 7 Dec. 2015, techcrunch.com/2015/12/07/jukedeck/?guccounter=1. <br>\n",
    "<a name=\"juke2\"></a>3.[^](#juke1note)Fishwick, Samuel. “Robot Rock: How AI Singstars Use Machine Learning to Write Harmonies.” The Standard, The Standard, 1 Mar. 2018, www.standard.co.uk/news/tech/jukedeck-maching-learning-ai-startup-music-a3779296.html. <br>\n",
    "<a name=\"juke3\"></a>4.[^](#juke2note)Ingham, Tim. “Tiktok Owner Bytedance Buys AI Music Company Jukedeck.” Music Business Worldwide, Music Business Worldwide, 23 July 2019, www.musicbusinessworldwide.com/tiktok-parent-bytedance-buys-ai-music-company-jukedeck/. <br>\n",
    "<a name=\"magenta\"></a>5.[^](#magentanote)Magenta, Google AI, magenta.tensorflow.org/. <br>\n",
    "<a name=\"tech\"></a>6.[^](#technote)Pal, Kaushik. “How Can an AI Model Create Music?” Techopedia, Techopedia, 18 Jan. 2024, www.techopedia.com/how-can-an-ai-model-create-music. <br>\n",
    "<a name=\"plaugicnote\"></a>7.[^](#plaugicnote): Plaugic, Lizzie. “Musician Taryn Southern on Composing Her New Album Entirely with AI.” The Verge, The Verge, 27 Aug. 2017, www.theverge.com/2017/8/27/16197196/taryn-southern-album-artificial-intelligence-interview.<br>\n",
    "<a name=\"sherburnenote\"></a>8.[^](#sherburnenote): Sherburne, Philip. “25 Microgenres That (Briefly) Defined the Last 25 Years.” Pitchfork, 7 Oct. 2021, pitchfork.com/features/lists-and-guides/microgenres-25th-anniversary/.<br>\n",
    "<a name=\"zacharynote\"></a>9.[^](#zachnote): Zachary. “How I Built a Lo-Fi Hip-Hop Music Generator.” Medium, Artificial Intelligence in Plain English, 17 Nov. 2020, ai.plainenglish.io/building-a-lo-fi-hip-hop-generator-e24a005d0144.<br>\n",
    "<a name=\"yangnote\"></a>10.[^](#yangnote): Yang, Li-Chia, and Alexander Lerch. “On the Evaluation of Generative Models in Music.” Neural Computing and Applications, vol. 32, no. 9, 3 Nov. 2018, pp. 4773–4784, https://doi.org/10.1007/s00521-018-3849-7.<br>\n",
    "<a name=\"browleenote\"></a>11.[^](#browleynote): Browlee, Jason. \"How to Implement the Frechet Inception Distance (FID) for Evaluating GANs\", 11 Oct. 2019, https://machinelearningmastery.com/how-to-implement-the-frechet-inception-distance-fid-from-scratch/.<br>\n",
    "<a name=\"rak\"></a>12.[^](#raknote)Rakshit, S. (2019, May 17). Classical music midi. Kaggle. https://www.kaggle.com/datasets/soumikrakshit/classical-music-midi <br>\n",
    "<a name=\"brow\"></a>13.[^](#browlnote)Lofi Records. (n.d.). Lofi Girl Releases Archive. Lofi Girl. https://lofigirl.com/releases/<br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert title here\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Brandon Kao\n",
    "- Kyle Vu\n",
    "- Connie Chang\n",
    "- Catherine Zhang\n",
    "- Grace Yang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This section should be short and clearly stated. It should be a single paragraph <200 words.  It should summarize: \n",
    "- what your goal/problem is\n",
    "- what the data used represents \n",
    "- the solution/what you did\n",
    "- major results you came up with (mention how results are measured) \n",
    "\n",
    "__NB:__ this final project form is much more report-like than the proposal and the checkpoint. Think in terms of writing a paper with bits of code in the middle to make the plots/tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Over the past 20 years, the development of electronic methods for music distribution and streaming services has decreased the barrier of entry into the music industry, giving rise to new genres and subcultures[<sup>[1]</sup>](#lorenznote). To account for this growth, extensive research has gone into automating genre classification using AI. In 2002, a study published in IEEE Transactions on Speech and Audio Processing trained a model on timbral texture, rhythmic content and pitch content of various songs[<sup>[2]</sup>](#lorenznote). When tasked with labeling songs with their genres, the model achieved an accuracy of 61%, which was comparable to the accuracy of humans[<sup>[2]</sup>](#lorenznote). Since then, models for music genre identification have grown, and are used by music streaming services to recommend tracks to their users. On Free Music Archive, a streaming service for royalty free music, a majority of their modes for genre identification employ Support Vector Classifiers(SVD), Logistic Regression(RNN), and Convolutional Neural Networks(CNN)[<sup>[3]</sup>](#lorenznote). Traditionally, genre classification has been approached in research with supervised learning models, which positions the label as an innate trait of a song.\n",
    "\n",
    "However, treating genre classification as a supervised learning problem fails to account for the evolving nature of music and art. Categorization can be difficult when genre definitions often vary from person to person or from their technical definitions[<sup>[2]</sup>](#lorenznote). The defining songs or traits of a genre can be hazy or ever-changing, and ‚Äúproblems occur if we take in account music genre categories as stable and essential entities‚Äù[<sup>[2]</sup>](#lorenznote). One example of how genre labels change is the removal of the broad label Latin from Spotify's genre classifier in 2022 in favor of more precise categories like Latin Pop, Urbano Latino, and Latin Hip Hop[<sup>[4]</sup>](#lorenznote). Some music genre identifier models have approached the problem using unsupervised learning. Spotify‚Äôs recommender system uses a song's audio data and its location in playlists to identity new genres, or create unique groupings that can not be described by an existing genre label[<sup>[5]</sup>](#lorenznote). Such methods attempt to accommodate how music labels are evolving and transient in nature. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "A challenge within the music industry is the ambiguity surrounding the classification of music genres.  Despite numerous attempts to delineate genre boundaries, there remains a notable absence of consensus on the defining characteristics of each genre. For instance, a single song may straddle multiple genres, such as pop and hip-hop, blurring traditional distinctions. This lack of clarity hinders effective communication and analysis within the field. A song contains many features such as, energy, loudness, tempo, speechiness, popularity, etc that can all be numerically quantified. However, alone, it is unclear what the correlations between each of those features are in regard to genre. Thus, we aim to create a model that clusters similar songs to see what features contribute to song similarity and bring distinct sound to a genre.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "[Dataset Link](https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset)\n",
    "\n",
    "The dataset ‚Äúüéπ Spotify Tracks Dataset‚Äù by user Maharshi Pandya on Kaggle compiles 114,000 tracks, or observations, from Spotify. Each track is measured on over 20 variables, which detail the track‚Äôs name, artist(s), audio features, sentiment, and more. The variable ‚Äútrack_genre‚Äù is the music genre the track belongs to. Since each observation can only be labeled by one genre, the dataset contains multiple entries of each track with duplicate information besides a different genre.The ‚Äúpopularity‚Äù column scores how trending a song is from 0 to 100, with 100 being the highest. The popularity is calculated by the amount of plays, with more recent plays being weighted higher. Other important variables track the qualities of each song on a scale from 0.0 to 1.0, such as ‚Äúdanceability‚Äù and ‚Äúvalence‚Äù to measure the danceability and positive mood of the song. \n",
    "\n",
    "We decided to limit our dataset to songs only belonging to the top 10 most popular genres and top 10 least popular genres (20 total). This was done in order to reduce the complexity of our dataset, as attempting to cluster around 114 unique genres would lead to high computational complexity and results that would be difficult to interpret. By selecting genres with a wide range of popularity, we are still able to get a varied representation of songs.\n",
    "\n",
    "For our data cleaning process, we took typical steps in order to get our data in the form we wanted. We started by removing all fully duplicate rows as well as rows with missing values. To input the data into the model, we also limited the dataset to the musical characteristic features, removing columns that were related to identification, like track_id or the album the song belonged to. We also excluded certain redundant/not useful features like whether or not a song was explicit rather than include it via a one-hot encoding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Provide enough detail (e.g., algorithmic description and/or theoretical properties) to convince us that your solution is applicable. Make sure to describe how the solution will be tested.  \n",
    "\n",
    "If you know details already, describe how (e.g., library used, function calls) you plan to implement the solution in a way that is reproducible.\n",
    "\n",
    "If it is appropriate to the problem statement, describe a benchmark model<a name=\"sota\"></a>[<sup>[3]</sup>](#sotanote) against which your solution will be compared. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "Propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "You may have done tons of work on this. Not all of it belongs here. \n",
    "\n",
    "Reports should have a __narrative__. Once you've looked through all your results over the quarter, decide on one main point and 2-4 secondary points you want us to understand. Include the detailed code and analysis results of those points only; you should spend more time/code/plots on your main point than the others.\n",
    "\n",
    "If you went down any blind alleys that you later decided to not pursue, please don't abuse the TAs time by throwing in 81 lines of code and 4 plots related to something you actually abandoned.  Consider deleting things that are not important to your narrative.  If its slightly relevant to the narrative or you just want us to know you tried something, you could keep it in by summarizing the result in this report in a sentence or two, moving the actual analysis to another file in your repo, and providing us a link to that file.\n",
    "\n",
    "### Subsection 1\n",
    "\n",
    "You will likely have different subsections as you go through your report. For instance you might start with an analysis of the dataset/problem and from there you might be able to draw out the kinds of algorithms that are / aren't appropriate to tackle the solution.  Or something else completely if this isn't the way your project works.\n",
    "\n",
    "### Subsection 2\n",
    "\n",
    "Another likely section is if you are doing any feature selection through cross-validation or hand-design/validation of features/transformations of the data\n",
    "\n",
    "### Subsection 3\n",
    "\n",
    "Probably you need to describe the base model and demonstrate its performance.  Maybe you include a learning curve to show whether you have enough data to do train/validate/test split or have to go to k-folds or LOOCV or ???\n",
    "\n",
    "### Subsection 4\n",
    "\n",
    "Perhaps some exploration of the model selection (hyper-parameters) or algorithm selection task. Validation curves, plots showing the variability of perfromance across folds of the cross-validation, etc. If you're doing one, the outcome of the null hypothesis test or parsimony principle check to show how you are selecting the best model.\n",
    "\n",
    "### Subsection 5 \n",
    "\n",
    "Maybe you do model selection again, but using a different kind of metric than before?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "OK, you've given us quite a bit of tech informaiton above, now its time to tell us what to pay attention to in all that.  Think clearly about your results, decide on one main point and 2-4 secondary points you want us to understand. Highlight HOW your results support those points.  You probably want 2-5 sentences per point.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "Are there any problems with the work?  For instance would more data change the nature of the problem? Would it be good to explore more hyperparams than you had time for?   \n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "If your project has obvious potential concerns with ethics or data privacy discuss that here.  Almost every ML project put into production can have ethical implications if you use your imagination. Use your imagination.\n",
    "\n",
    "Even if you can't come up with an obvious ethical concern that should be addressed, you should know that a large number of ML projects that go into producation have unintended consequences and ethical problems once in production. How will your team address these issues?\n",
    "\n",
    "Consider a tool to help you address the potential issues such as https://deon.drivendata.org\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Reiterate your main point and in just a few sentences tell us how your results support it. Mention how this work would fit in the background/context of other work in this field if you can. Suggest directions for future work if you want to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): Lorenz, T. (9 Dec 2021) Birds Aren‚Äôt Real, or Are They? Inside a Gen Z Conspiracy Theory. *The New York Times*. https://www.nytimes.com/2021/12/09/technology/birds-arent-real-gen-z-misinformation.html<br> \n",
    "<a name=\"admonishnote\"></a>2.[^](#admonish): Also refs should be important to the background, not some randomly chosen vaguely related stuff. Include a web link if possible in refs as above.<br>\n",
    "<a name=\"sotanote\"></a>3.[^](#sota): Perhaps the current state of the art solution such as you see on [Papers with code](https://paperswithcode.com/sota). Or maybe not SOTA, but rather a standard textbook/Kaggle solution to this kind of problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
